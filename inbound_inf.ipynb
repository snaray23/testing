{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c0136-c113-4bda-a125-9324cc44cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _data_input():\n",
    "    \n",
    "    query = f\"\"\"select\n",
    "    r2d2_sub_category_id, r2d2_division_id, r2d2_super_department_id, r2d2_department_id, r2d2_category_id,rpt_lvl_0_nm, rpt_lvl_1_nm, rpt_lvl_2_nm, rpt_lvl_3_nm, rpt_lvl_4_nm, wm_yr_wk,\n",
    "    sum(net_cncl_qty) net_cncl_qty, \n",
    "    avg(unit_price_amt) unit_price_amt,\n",
    "    sum(unit_price_amt*net_cncl_qty)/sum(net_cncl_qty) as unit_price_amt_w, \n",
    "    avg(volume_cu_ft*net_cncl_qty)/sum(net_cncl_qty) volume_cu_ft_w, \n",
    "    extract(year from order_plcd_dt) as purchase_year,\n",
    "    extract(month from order_plcd_dt) as purchase_month, \n",
    "    from wmt-one-demand-dev.inventory_placement_sn.historic_sales_nonreplen_2022_2025_3yr_sales_v6 a\n",
    "    where   \n",
    "    (r2d2_sub_category_id is not null or r2d2_division_id is not null \n",
    "    or r2d2_super_department_id is not null or r2d2_department_id is not null \n",
    "    or  r2d2_category_id is not null)\n",
    "    group by r2d2_sub_category_id, r2d2_division_id, r2d2_super_department_id, r2d2_department_id, r2d2_category_id, rpt_lvl_0_nm, rpt_lvl_1_nm, rpt_lvl_2_nm, rpt_lvl_3_nm, rpt_lvl_4_nm, wm_yr_wk, extract(year from order_plcd_dt), extract(month from order_plcd_dt)\"\"\"\n",
    "    sales_df = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    \n",
    "    query = f\"\"\" Select * except(inbound), inbound as calc_inbound\n",
    "        from \n",
    "        -- wmt-one-demand-dev.inventory_placement_sn.inbound_3yrs_new_nr\n",
    "        wmt-one-demand-dev.inventory_placement_sn.inbound_3yrs_new_nr_v2\n",
    "        -- wmt-one-demand-dev.inventory_placement_sn.inbound_3yrs_new_nr_v2\n",
    "        -- wmt-one-demand-dev.inventory_placement_sn.inbound_3yrs_new_nr_v3\n",
    "        where 1 = 1\n",
    "        -- and r2d2_division_name not in ('UNASSIGNED','UNASSIGNED L0')\n",
    "        and inbound >=0\n",
    "        \"\"\"\n",
    "    inbound = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    inbound = inbound.rename(columns = {'WM_YR_WK': 'wm_yr_wk'})\n",
    "    \n",
    "    query = f\"\"\"SELECT distinct max(wm_month_nbr) month, max(wm_year_nbr) year, WmYrWk\n",
    "            -- extract(year from min(WM_DATE)) year, extract(month from min(WM_DATE)) month\n",
    "            FROM wmt-euclid-prod.geodemand.GeoCalendar\n",
    "            where CalendarDate between '2022-01-01' and current_date()\n",
    "            group by WmYrWk\"\"\"\n",
    "    cal_df = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    cal_df = cal_df.rename(columns = {'WmYrWk':'wm_yr_wk'})\n",
    "    \n",
    "    query = f\"\"\"SELECT distinct EventType, EventName, WmYrWk \n",
    "            from `wmt-euclid-prod.geodemand.GeoEvent`\n",
    "            where Year in (2022,2023,2024,2025)\n",
    "            and EventType in ('National', 'Cultural', 'Sporting')\"\"\"\n",
    "    event_df = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    event_df = event_df.rename(columns = {'WmYrWk': 'wm_yr_wk'})\n",
    "    \n",
    "    query = f\"\"\"SELECT * \n",
    "            from wmt-one-demand-dev.inventory_placement_sn.historic_inv_nonreplen_2022_2025_3yr_v7\n",
    "            -- wmt-one-demand-dev.inventory_placement_sn.historic_inv_nonreplen_2022_2024_3yr_v5\n",
    "            -- wmt-one-demand-dev.inventory_placement_sn.historic_inv_nonreplen_2022_2024_3yr_v4\n",
    "            -- wmt-one-demand-dev.inventory_placement_sn.historic_inv_nonreplen_2022_2024_3yr_v1\n",
    "            where r2d2_sub_category_id is not null and r2d2_division_id is not null and r2d2_super_department_id is not null\n",
    "            and r2d2_department_id is not null and r2d2_category_id is not null\"\"\"\n",
    "    inv_df = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    inv_df = inv_df.rename(columns = {'WmYrWk': 'wm_yr_wk'})\n",
    "\n",
    "    query = f\"\"\"SELECT distinct\n",
    "\t\t\t r2d2_division_id\n",
    "\t\t\t, r2d2_super_department_id\n",
    "\t\t\t, r2d2_department_id\n",
    "\t\t\t, r2d2_category_id\n",
    "\t\t\t, r2d2_sub_category_id\n",
    "\t\t\t, network_type\n",
    "\t\t\t, processed_time\n",
    "\t\t\t--, SUM(CASE WHEN apparel_flag = TRUE THEN 1 ELSE 0 END) apparel\n",
    "\t\t\t--, sum(case when is_sortable = true and apparel_flag = False then 1 else 0 end) sort\n",
    "\t\t\t--, sum(case when is_sortable = false and apparel_flag = False then 1 else 0 end) non_sort\n",
    "\t\t\tFROM \n",
    "\t\t\twmt-sg-prod.us_wmt_placements.item_attributes_v2\n",
    "\t\t\tWHERE is_replen = FALSE\n",
    "\t\t\tand r2d2_sub_category_id is not null\n",
    "\t\t\tand processed_time = (Select max(processed_time) from wmt-sg-prod.us_wmt_placements.item_attributes_v2)\n",
    "\t\t\t;\"\"\"\n",
    "\titem_univ = read_bq(\"wmt-one-demand-dev\", query)\n",
    "    \n",
    "    return sales_df,inbound, cal_df, event_df, inv_df, item_univ\n",
    "\n",
    "def _data_prep(sales_df,inbound, cal_df, inv_df, event_df, group_cols) :\n",
    "    \n",
    "    sales_weekly = sales_df.groupby(group_cols, \n",
    "                                    as_index = False).agg(sale_qty = ('net_cncl_qty','sum'), \n",
    "                                                          unit_price_amt = ('unit_price_amt','mean'),\n",
    "                                                          unit_price_amt_w = ('unit_price_amt_w','mean'),\n",
    "                                                          volume_cu_ft_w = ('volume_cu_ft_w','median')\n",
    "                                                         )\n",
    "    inbound = inbound.groupby(group_cols+ ['year','month'], \n",
    "                               as_index = False).agg(tot_capacity = ('tot_capacity','sum'),\n",
    "                                                     calc_inbound = ('calc_inbound','sum'),\n",
    "                                                     order_qty = ('orderqty', 'sum')\n",
    "                                                    )\n",
    "    inv_df = inv_df.groupby(group_cols, as_index = False).agg(DaysInv = ('DaysInv','mean'),\n",
    "                                                              DaysOOS = ('DaysOOS','mean'),\n",
    "                                                              InvQty_Sun = ('InvQty_Sun','sum'),\n",
    "                                                              InvQty_Mon = ('InvQty_Mon','sum'),\n",
    "                                                              InvQty_Tue = ('InvQty_Tue','sum'),\n",
    "                                                              InvQty_Wed = ('InvQty_Wed','sum'),\n",
    "                                                              InvQty_Thu = ('InvQty_Thu','sum'),\n",
    "                                                              InvQty_Fri = ('InvQty_Fri','sum'),\n",
    "                                                              InvQty_Sat = ('InvQty_Sat','sum')\n",
    "                                                             )\n",
    "    event_df['value'] = 1 \n",
    "    event_df_pivot = event_df.pivot_table(index = 'wm_yr_wk', columns = 'EventType', values = \"value\", fill_value=0).reset_index()\n",
    "    event_df_pivot['is_event'] = event_df_pivot[['National', 'Cultural', 'Sporting']].max(axis=1)\n",
    "    event_df_pivot.columns.name = None\n",
    "    \n",
    "    sales_weekly['sale_start_wk'] = sales_weekly.groupby(cat_group)['wm_yr_wk'].transform('min')\n",
    "    inbound['inbound_start_wk'] = inbound.groupby(cat_group)['wm_yr_wk'].transform('min')\n",
    "    inbound['inbound_end_wk'] = inbound.groupby(cat_group)['wm_yr_wk'].transform('max')\n",
    "    inv_df['inv_start_wk'] = inv_df.groupby(cat_group)['wm_yr_wk'].transform('min')\n",
    "    \n",
    "    sales_inbound = sales_weekly.merge(inbound, on = group_cols, how = 'outer')\n",
    "    sales_inbound_inv = inv_df.merge(sales_inbound, on = group_cols, how = 'outer')\n",
    "    categories = inbound[cat_group].drop_duplicates()\n",
    "    cal_df = cal_df.merge(event_df_pivot, on = 'wm_yr_wk', how = 'left')\n",
    "    cal_categories = pd.merge(categories, cal_df, how = 'cross')\n",
    "    df_merge = pd.merge(cal_categories, sales_inbound_inv, how = 'left', on = group_cols)\n",
    "    \n",
    "    df_merge = df_merge.drop(columns = ['month_y','year_y']).rename(columns = {'month_x':'month','year_x':'year'})\n",
    "    df = df_merge[group_cols+ ['DaysInv','DaysOOS',  'InvQty_Sun', 'InvQty_Mon', \n",
    "#                                'DaysInv_subcat','DaysOOS_subcat', 'DaysInv_old', 'DaysOOS_old',\n",
    "                               'InvQty_Tue', 'InvQty_Wed','InvQty_Thu', 'InvQty_Fri', 'InvQty_Sat', 'inv_start_wk',\n",
    "                               'sale_qty', 'unit_price_amt', 'unit_price_amt_w', 'sale_start_wk', 'volume_cu_ft_w',\n",
    "                               'inbound_start_wk', 'inbound_end_wk', 'tot_capacity','calc_inbound', 'year', 'month', \n",
    "                               'is_event', 'order_qty']]\n",
    "#     df['start_wk'] = np.min(df[['sale_start_wk','inbound_start_wk','inv_start_wk']], axis = 1)\n",
    "    df['start_wk'] = df.groupby(cat_group)['inbound_start_wk'].transform('min')\n",
    "    df['end_wk'] = df.groupby(cat_group)['inbound_end_wk'].transform('max')\n",
    "    df = df[(df['wm_yr_wk'] >= df['start_wk']) & (df['wm_yr_wk'] <= df['end_wk'])]\n",
    "    # include  seasonality\n",
    "    cum_sum = df.groupby(cat_group+['year'], as_index = False).agg(net_yr_sales = ('sale_qty','sum'),\n",
    "                                                                   net_yr_ib = ('calc_inbound','sum')\n",
    "                                                                  )\n",
    "    cum_sum = df.merge(cum_sum, on = cat_group + ['year'])\n",
    "    cum_sum['seasonality_yr_sales'] = np.round(cum_sum['sale_qty']/cum_sum['net_yr_sales'],3)\n",
    "    cum_sum['seasonality_yr_ib'] = np.round(cum_sum['calc_inbound'].astype('float')/cum_sum['net_yr_ib'].astype('float'),3)\n",
    "    \n",
    "    \n",
    "    cum_sum1 = cum_sum.groupby(cat_group, as_index = False).agg(net_sales = ('sale_qty','sum'),\n",
    "                                                                net_ib = ('calc_inbound','sum')\n",
    "                                                         )\n",
    "    cum_sum1 = cum_sum.merge(cum_sum1, on = cat_group)\n",
    "    cum_sum1['seasonality_sales'] = np.round(cum_sum1['sale_qty']/cum_sum1['net_sales'],3)\n",
    "    cum_sum1['seasonality_ib'] = np.round(cum_sum1['calc_inbound'].astype('float')/cum_sum1['net_ib'].astype('float'),3)\n",
    "    \n",
    "    df = cum_sum1[group_cols + ['DaysInv','DaysOOS', \n",
    "#                                 'DaysInv_subcat','DaysOOS_subcat', 'DaysInv_old', 'DaysOOS_old', \n",
    "                                'InvQty_Sun', 'InvQty_Mon', 'InvQty_Tue', \n",
    "                               'InvQty_Wed','InvQty_Thu', 'InvQty_Fri', 'InvQty_Sat',\n",
    "                               'sale_qty', 'unit_price_amt', 'unit_price_amt_w', 'volume_cu_ft_w',\n",
    "                                'tot_capacity','calc_inbound', 'year', 'month', 'order_qty',\n",
    "                                'is_event',\n",
    "                                'seasonality_sales', 'seasonality_ib', 'seasonality_yr_sales', 'seasonality_yr_ib',\n",
    "                              ]]\n",
    "    cols = ['DaysInv','DaysOOS', 'InvQty_Sun', 'InvQty_Mon', 'InvQty_Tue', 'InvQty_Wed','InvQty_Thu', 'InvQty_Fri', 'InvQty_Sat',\n",
    "#             'DaysInv_subcat','DaysOOS_subcat', 'DaysInv_old', 'DaysOOS_old', \n",
    "            'sale_qty', 'unit_price_amt', 'unit_price_amt_w', 'volume_cu_ft_w',\n",
    "            'tot_capacity','calc_inbound','order_qty',\n",
    "            'seasonality_sales', 'seasonality_ib', 'seasonality_yr_sales', 'seasonality_yr_ib'\n",
    "           ]\n",
    "    df[cols] =  df[cols].astype('float').round(3)\n",
    "    df = df.sort_values(by = group_cols)\n",
    "    return df\n",
    "\n",
    "def _data_cleaning(df_missing):\n",
    "    print('Missing inbound values:',round(df_missing['calc_inbound'].isna().sum()*100/len(df_missing),3))\n",
    "    print('Zero inbound values:',round(len(df_missing[df_missing['calc_inbound']==0])*100/len(df_missing),3))\n",
    "    print('Missing order qty values:',round(df_missing['order_qty'].isna().sum()*100/len(df_missing),3))\n",
    "    print('Zero order qty values:',round(len(df_missing[df_missing['order_qty']==0])*100/len(df_missing),3))\n",
    "    \n",
    "    # adding inventory and last year week columns\n",
    "    df_missing['is_event'] = df_missing['is_event'].fillna(0)\n",
    "    df_missing['last_year_wk'] = df_missing['wm_yr_wk']-100\n",
    "    df_missing['OH_Inv'] = df_missing['InvQty_Fri'].fillna(df_missing['InvQty_Thu']).fillna(df_missing['InvQty_Wed']).fillna(df_missing['InvQty_Tue']).fillna(df_missing['InvQty_Mon']).fillna(df_missing['InvQty_Sun']).fillna(df_missing['InvQty_Sat'])\n",
    "\n",
    "    ## outlier removal\n",
    "    sale_qty_z = (df_missing.sale_qty- np.nanmean(df_missing.sale_qty))/np.nanstd(df_missing.sale_qty)\n",
    "#     print(sale_qty_z[~sale_qty_z.isna()][sale_qty_z>3].quantile(.996))\n",
    "    index_temp = sale_qty_z[sale_qty_z > sale_qty_z[~sale_qty_z.isna()][sale_qty_z>3].quantile(.996)].index\n",
    "    subcat_list_sale = df_missing.iloc[index_temp].r2d2_sub_category_id.unique()\n",
    "    print('sales:', len(subcat_list_sale),len(df_missing[~df_missing.r2d2_sub_category_id.isin(subcat_list_sale)])/len(df_missing))\n",
    "\n",
    "    inv_z = (df_missing[~df_missing['OH_Inv'].isna()]['OH_Inv'] - df_missing[~df_missing['OH_Inv'].isna()]['OH_Inv'].mean())/df_missing[~df_missing['OH_Inv'].isna()]['OH_Inv'].std()\n",
    "    index_temp = inv_z[inv_z > 5].index\n",
    "    subcat_list_inv = df_missing.iloc[index_temp].r2d2_sub_category_id.unique()\n",
    "    print('inventory:', len(subcat_list_inv),len(df_missing[~df_missing.r2d2_sub_category_id.isin(subcat_list_inv)])/len(df_missing))\n",
    "\n",
    "    calc_inbound_z = (df_missing[~df_missing.calc_inbound.isna()]['calc_inbound']- df_missing[~df_missing.calc_inbound.isna()]['calc_inbound'].mean())/df_missing[~df_missing.calc_inbound.isna()]['calc_inbound'].std()\n",
    "\n",
    "#     print(calc_inbound_z[~calc_inbound_z.isna()][calc_inbound_z>3].quantile(.95))\n",
    "    index_temp = calc_inbound_z[calc_inbound_z > calc_inbound_z[~calc_inbound_z.isna()][calc_inbound_z>3].quantile(.95)].index\n",
    "    subcat_list_calc = df_missing.iloc[index_temp].r2d2_sub_category_id.unique()\n",
    "    print('inbound:', len(subcat_list_calc),len(df_missing[~df_missing.r2d2_sub_category_id.isin(subcat_list_calc)])/len(df_missing))\n",
    "    \n",
    "    # combining\n",
    "    subcat_list = pd.concat([pd.Series(subcat_list_calc), pd.Series(subcat_list_inv), pd.Series(subcat_list_sale)])\n",
    "    subcat_list = subcat_list.unique()\n",
    "    print('subcat exclusion list:', subcat_list)\n",
    "\n",
    "    ## Missing value imputation\n",
    "    df_missing = df_missing[~df_missing['r2d2_sub_category_id'].isin(subcat_list)]\n",
    "\n",
    "    if len(df_missing[(df_missing['OH_Inv'].isna()) & (df_missing['sale_qty'] > 0)]) >0:\n",
    "        mask = df_missing['OH_Inv'].isna() & (df_missing['sale_qty'] > 0)\n",
    "\n",
    "        # Forward fill missing OH_Inv within each subcategory only for relevant rows\n",
    "        df_missing.loc[mask, 'OH_Inv'] = df_missing.groupby(cat_group)['OH_Inv'].ffill()\n",
    "        df_missing.loc[mask, 'DaysOOS'] = df_missing.groupby(cat_group)['DaysOOS'].ffill()\n",
    "        df_missing.loc[mask, 'DaysInv'] = df_missing.groupby(cat_group)['DaysInv'].ffill()\n",
    "\n",
    "        print(len(df_missing[(df_missing['OH_Inv'].isna()) & (df_missing['sale_qty'] > 0)]))\n",
    "\n",
    "    if df_missing['OH_Inv'].isna().sum() > 0 and (df_missing['calc_inbound'] > 0).any():\n",
    "        mask = df_missing['OH_Inv'].isna() & (df_missing['calc_inbound'] > 0)\n",
    "\n",
    "        # Apply shift only once per group and maintain DataFrame shape\n",
    "        df_missing['prev_OH_Inv'] = df_missing.groupby(cat_group)['OH_Inv'].transform(lambda x: x.shift(1))\n",
    "        df_missing['prev_sale_qty'] = df_missing.groupby(cat_group)['sale_qty'].transform(lambda x: x.shift(1))\n",
    "\n",
    "        # Compute new OH_Inv values only for missing ones\n",
    "        df_missing.loc[mask, 'OH_Inv'] = (df_missing['prev_OH_Inv'].fillna(0) \n",
    "                                          - df_missing['prev_sale_qty'].fillna(0) \n",
    "                                          + df_missing['calc_inbound'])\n",
    "\n",
    "        df_missing.drop(columns = ['prev_OH_Inv','prev_sale_qty'], inplace = True)\n",
    "        print(\"Remaining NaNs:\", df_missing.loc[mask, 'OH_Inv'].isna().sum())\n",
    "\n",
    "    if len(df_missing[(df_missing['OH_Inv'] == 0) & (df_missing['DaysInv'].isna())])>0:\n",
    "        mask = (df_missing['OH_Inv'] == 0) & (df_missing['DaysInv'].isna())\n",
    "        df_missing.loc[mask,'DaysInv'] = 0\n",
    "        df_missing.loc[mask,'DaysOOS'] = 7\n",
    "\n",
    "    ## not used in this version\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(cat_group)['unit_price_amt'].transform(lambda x: x.rolling(window = 5, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt'] = df_missing['unit_price_amt'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "    print(df_missing['unit_price_amt'].isna().sum())\n",
    "\n",
    "    df_missing['unit_price_amt'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id', 'r2d2_category_id', 'wm_yr_wk'])[\n",
    "        'unit_price_amt'].transform(lambda group: group.fillna(group.mean()))\n",
    "    print(df_missing['unit_price_amt'].isna().sum())\n",
    "\n",
    "    ## not used in this version\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(cat_group)['unit_price_amt'].transform(lambda x: x.rolling(window = 5, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt'] = df_missing['unit_price_amt'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "    df_missing['unit_price_amt'].isna().sum()\n",
    "\n",
    "    df_missing['unit_price_amt'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id', 'wm_yr_wk'])[\n",
    "        'unit_price_amt'].transform(lambda group: group.fillna(group.mean()))\n",
    "    df_missing['unit_price_amt'].isna().sum()\n",
    "\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id']\n",
    "                                                    )['unit_price_amt'].transform(lambda x: x.rolling(window = 104, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt'] = df_missing['unit_price_amt'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "    df_missing['unit_price_amt'].isna().sum()\n",
    "\n",
    "    ## unit_price_amt_w_w & unit_price_amt_w\n",
    "    # not used in this version\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(cat_group)['unit_price_amt_w'].transform(lambda x: x.rolling(window = 5, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt_w'] = df_missing['unit_price_amt_w'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "\n",
    "    df_missing[['unit_price_amt_w']].isna().sum()\n",
    "\n",
    "    df_missing['unit_price_amt_w'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id', 'r2d2_category_id', 'wm_yr_wk'])[\n",
    "        'unit_price_amt_w'].transform(lambda group: group.fillna(group.mean()))\n",
    "    df_missing[['unit_price_amt_w']].isna().sum()\n",
    "\n",
    "    ## unit_price_amt_w_w & unit_price_amt_w\n",
    "    # not used in this version\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(cat_group)['unit_price_amt_w'].transform(lambda x: x.rolling(window = 5, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt_w'] = df_missing['unit_price_amt_w'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "\n",
    "    df_missing[['unit_price_amt_w']].isna().sum()\n",
    "\n",
    "    df_missing['unit_price_amt_w'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id', 'wm_yr_wk'])[\n",
    "        'unit_price_amt_w'].transform(lambda group: group.fillna(group.mean()))\n",
    "    df_missing[['unit_price_amt_w']].isna().sum()\n",
    "\n",
    "    ## unit_price_amt_w_w & unit_price_amt_w\n",
    "    # not used in this version\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(cat_group)['unit_price_amt_w'].transform(lambda x: x.rolling(window = 5, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt_w'] = df_missing['unit_price_amt_w'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "\n",
    "    df_missing[['unit_price_amt_w']].isna().sum()\n",
    "\n",
    "    df_missing = df_missing.sort_values(by = group_cols)\n",
    "    df_missing['rolling_price'] = df_missing.groupby(['r2d2_division_id', 'r2d2_super_department_id',\n",
    "                                                       'r2d2_department_id']\n",
    "                                                    )['unit_price_amt'].transform(lambda x: x.rolling(window = 104, min_periods = 1).median())\n",
    "    df_missing['unit_price_amt'] = df_missing['unit_price_amt'].fillna(df_missing['rolling_price'])\n",
    "    df_missing = df_missing.drop(columns = ['rolling_price'])\n",
    "    df_missing['unit_price_amt'].isna().sum()\n",
    "\n",
    "    # grouped = df_missing.groupby(cat_group)\n",
    "    def mapping_prev_year(grouped):\n",
    "        grouped['last_year_wk'] = grouped['wm_yr_wk']-100\n",
    "        grouped['last_2_year_wk'] = grouped['wm_yr_wk']-200\n",
    "        last_yr_map = grouped.set_index('wm_yr_wk')[['sale_qty','DaysOOS']]\n",
    "        grouped['last_yr_sale_qty'] = grouped['last_year_wk'].map(last_yr_map['sale_qty'])\n",
    "        grouped['last_2yr_sale_qty'] = grouped['last_2_year_wk'].map(last_yr_map['sale_qty'])\n",
    "        grouped['last_yr_DaysOOS'] = grouped['last_year_wk'].map(last_yr_map['DaysOOS'])\n",
    "        grouped['last_2yr_DaysOOS'] = grouped['last_2_year_wk'].map(last_yr_map['DaysOOS'])\n",
    "\n",
    "        grouped['sale_qty_new'] = np.where(\n",
    "            grouped['DaysOOS'] < 3, \n",
    "            grouped['sale_qty'],  # Keep original sale_qty if DaysOOS < 3\n",
    "            np.where(\n",
    "                (grouped['DaysOOS'] >= 3) & \n",
    "                (grouped['last_yr_DaysOOS'] < 3) & \n",
    "                grouped['last_yr_sale_qty'].notna(),\n",
    "                grouped['last_yr_sale_qty'],  # Use last year's sales if last_yr_DaysOOS < 3\n",
    "                np.where(\n",
    "                    (grouped['DaysOOS'] >= 3) & \n",
    "                    (grouped['last_yr_DaysOOS'] >= 3) & \n",
    "                    (grouped['last_2yr_DaysOOS'] < 3) & \n",
    "                    grouped['last_2yr_sale_qty'].notna(),\n",
    "                    grouped['last_2yr_sale_qty'],  # Use last 2 years' sales if conditions met\n",
    "                    grouped['sale_qty']  # Otherwise, keep the original sale_qty\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        grouped.drop(columns = ['last_2_year_wk','last_yr_sale_qty','last_2yr_sale_qty',\n",
    "                                'last_yr_DaysOOS','last_2yr_DaysOOS'], inplace = True)\n",
    "        return grouped\n",
    "\n",
    "    df_missing = df_missing.groupby(cat_group, group_keys=False).apply(mapping_prev_year)\n",
    "    # df_missing['sale_qty_new'] = df_missing['sale_qty_new'].fillna(0)\n",
    "    df_missing['sale_qty_new'].isna().sum()\n",
    "   \n",
    "    return df_missing\n",
    "\n",
    "def _build_feature(df, group_col, lag_features, lead_features, lags, leads, ly_features, target_feature, forecast_horizon, rolling_windows, time_col):\n",
    "    \n",
    "    df = df.sort_values(by = time_col)\n",
    "    \n",
    "    # Function to create lag features for any column\n",
    "    def create_lag_features(df, column, group_col, lags):\n",
    "        for lag in lags:\n",
    "            df[f'{column}_lag_{lag}'] = df.groupby(group_col)[column].shift(lag)\n",
    "        return df\n",
    "    # Function to create lag features for any column\n",
    "    def create_lead_features(df, column, group_col, leads):\n",
    "        for lead in leads:\n",
    "            df[f'{column}_lead_{lead}'] = df.groupby(group_col)[column].shift(-lead)\n",
    "        return df\n",
    "    # Function to create difference features for any column\n",
    "    def create_diff_features(df, column, group_col, diffs):\n",
    "        for diff in diffs:\n",
    "            df[f'{column}_diff_{diff}'] = df.groupby(group_col)[column].diff(diff)\n",
    "        return df\n",
    "    # Function to create rolling averages (e.g., 4-week moving average)\n",
    "    def create_rolling_average(df, column, group_col, weeks):\n",
    "        for week in weeks:\n",
    "            df[f'{column}_rolling_{week}w'] = df.groupby(group_col)[column].transform(lambda x: x.rolling(week).mean())\n",
    "        return df\n",
    "    # Function to create cumulative sum features for any column\n",
    "    def create_cumsum_features(df, column, group_col):\n",
    "        df[f'{column}_cumsum'] = df.groupby(group_col)[column].cumsum()\n",
    "        return df\n",
    "    \n",
    "    # Function to create cumulative sum features for any column\n",
    "    def create_ly_features(df, column, group_col):\n",
    "        df_grouped = df.groupby(group_col, group_keys = False)\n",
    "        df['last_2year_wk'] = df['wm_yr_wk']-200\n",
    "        \n",
    "        def create_features(group):\n",
    "            last_yr_map = group.set_index('wm_yr_wk')[column].to_dict()\n",
    "            \n",
    "            group[f'{column}_ly_cur_wk'] = group['last_year_wk'].map(last_yr_map)\n",
    "            group[f'{column}_l2y_cur_wk'] = group['last_2year_wk'].map(last_yr_map)\n",
    "            \n",
    "            for lag in range(1,4):\n",
    "                group[f'{column}_ly_cur_wk_lag_{lag}'] = group['last_year_wk'\n",
    "                                                              ].map(lambda x:last_yr_map.get(x-lag, None))\n",
    "                group[f'{column}_l2y_cur_wk_lag_{lag}'] = group['last_2year_wk'\n",
    "                                                              ].map(lambda x:last_yr_map.get(x-lag, None))\n",
    "            for lead in range(1,4):\n",
    "                group[f'{column}_ly_cur_wk_lead_{lead}'] = group['last_year_wk'\n",
    "                                                              ].map(lambda x:last_yr_map.get(x+lead, None))\n",
    "                group[f'{column}_l2y_cur_wk_lead_{lead}'] = group['last_2year_wk'\n",
    "                                                              ].map(lambda x:last_yr_map.get(x+lead, None))\n",
    "            \n",
    "            return group\n",
    "        \n",
    "        df = df_grouped.apply(create_features)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def add_frequency_features(df, group_col, time_col, value_col):\n",
    "        \"\"\"\n",
    "        Adds frequency domain features to a time-series DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - df: Input DataFrame.\n",
    "        - group_col: List of columns to group by (e.g., hierarchical IDs).\n",
    "        - time_col: Column representing time (e.g., 'wm_yr_wk').\n",
    "        - value_col: Column for which frequency domain features will be computed (e.g., 'calc_inbound').\n",
    "\n",
    "        Returns:\n",
    "        - Updated DataFrame with frequency features.\n",
    "        \"\"\"\n",
    "        def extract_fft_features(group):\n",
    "            # Ensure the group is sorted by time\n",
    "            group = group.sort_values(by=time_col)\n",
    "\n",
    "            # Fill missing values with 0 for FFT computation\n",
    "            signal = group[value_col].fillna(0).values\n",
    "\n",
    "            # Apply FFT\n",
    "            fft_result = np.fft.fft(signal)\n",
    "            fft_magnitude = np.abs(fft_result)  # Magnitude of the FFT\n",
    "            fft_freqs = np.fft.fftfreq(len(signal))  # Corresponding frequencies\n",
    "\n",
    "            # Extract features\n",
    "            spectral_energy = np.sum(fft_magnitude**2)  # Total energy\n",
    "            dominant_frequency = fft_freqs[np.argmax(fft_magnitude[:len(fft_freqs)//2])]  # Dominant frequency\n",
    "            dominant_amplitude = np.max(fft_magnitude[:len(fft_freqs)//2])  # Amplitude of the dominant frequency\n",
    "            spectral_entropy = -np.sum((fft_magnitude**2 / spectral_energy) * np.log(fft_magnitude**2 / spectral_energy + 1e-8))\n",
    "\n",
    "            # Add features as new columns\n",
    "            group[f'{value_col}_spectral_energy'] = spectral_energy\n",
    "            group[f'{value_col}_dominant_frequency'] = dominant_frequency\n",
    "            group[f'{value_col}_dominant_amplitude'] = dominant_amplitude\n",
    "            group[f'{value_col}_spectral_entropy'] = spectral_entropy\n",
    "\n",
    "            return group\n",
    "    \n",
    "        # Apply the function group-wise\n",
    "        \n",
    "        valid_grp = df.groupby(group_col)[value_col].count()\n",
    "        valid_grp = valid_grp[valid_grp>1].index\n",
    "        df = df[df[group_col].apply(tuple, axis=1).isin(valid_grp)]\n",
    "        \n",
    "        df = df.groupby(group_col, group_keys=False).apply(extract_fft_features)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "#     leads = np.arange(1, 13, 1) # 1-week, 2-week, ... 52-week lags\n",
    "    diffs = [1, 2, 4] # Week-over-week, 2-week difference, and 4-week difference\n",
    "    \n",
    "    # Create lag, lead, and difference features\n",
    "    for col in lag_features:\n",
    "        df = create_lag_features(df, col, group_col, lags)\n",
    "        df = create_diff_features(df, col, group_col, diffs)\n",
    "        df = create_rolling_average(df, col, group_col, rolling_windows)\n",
    "\n",
    "    for col in lead_features:\n",
    "        df = create_lead_features(df, col, group_col, leads)\n",
    "    \n",
    "    for col in target_feature:\n",
    "        df = create_lead_features(df, col, group_col, forecast_horizon)\n",
    "    \n",
    "    for col in ly_features:\n",
    "        df = create_ly_features(df, col, group_col)\n",
    "\n",
    "    for col in ['calc_inbound']:\n",
    "        df = add_frequency_features(df, group_col, 'wm_yr_wk', col)\n",
    "    \n",
    "    print(f\"Columns after feature engineering: {df.columns}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_hierarchical_features(\n",
    "    df,\n",
    "    hierarchy_cols,\n",
    "    time_col,\n",
    "    features_to_agg,\n",
    "    aggs=[\"mean\", \"sum\"],\n",
    "    rolling_windows=[4],\n",
    "    lags=[1],\n",
    "    sort_col=None,\n",
    "    prevent_leakage=True\n",
    "):\n",
    "\n",
    "    df_out = df.copy()\n",
    "    sort_col = sort_col or time_col\n",
    "\n",
    "    for level in hierarchy_cols[1:]:  # Skip most granular level\n",
    "        print(f\"\\n Processing level: {level}\")\n",
    "\n",
    "        for feat in features_to_agg:\n",
    "            # Sort before groupby\n",
    "            df_sorted = df.sort_values([level, sort_col])\n",
    "\n",
    "            # Optional leakage protection via .shift(1)\n",
    "            shifted_series = (\n",
    "                df_sorted.groupby(level)[feat].shift(1)\n",
    "                if prevent_leakage else\n",
    "                df_sorted[feat]\n",
    "            )\n",
    "\n",
    "            df_shifted = df_sorted.copy()\n",
    "            df_shifted[f\"{feat}_shifted\"] = shifted_series\n",
    "\n",
    "            # --- Aggregations ---\n",
    "            print(f\" Aggregating {feat} with {aggs}\")\n",
    "            agg_df = (\n",
    "                df_shifted.groupby([level, time_col])[f\"{feat}_shifted\"]\n",
    "                .agg(aggs)\n",
    "                .reset_index()\n",
    "            )\n",
    "            # Rename columns\n",
    "            agg_df.columns = [level, time_col] + [f\"{level}_{feat}_{agg}\" for agg in aggs]\n",
    "\n",
    "            # Merge into output\n",
    "            df_out = df_out.merge(agg_df, on=[level, time_col], how='left')\n",
    "            \n",
    "            for window in rolling_windows:\n",
    "            \n",
    "                print(f\" Rolling mean (window={window}) for {feat}\")\n",
    "\n",
    "                rolling_col = f\"{level}_{feat}_rolling{window}w\"\n",
    "\n",
    "                df_out[rolling_col] = (\n",
    "                    df.sort_values([level, sort_col])\n",
    "                    .groupby(level)[feat]\n",
    "                    .transform(lambda x: x.shift(1).rolling(window=window, min_periods=1).mean())\n",
    "                )\n",
    "            \n",
    "\n",
    "            # --- Lag Features ---\n",
    "            for lag in lags:\n",
    "                print(f\" Lag {lag} for {feat}\")\n",
    "                df_out[f\"{level}_{feat}_lag{lag}w\"] = (\n",
    "                    df_sorted.groupby(level)[feat].shift(lag)\n",
    "                ).values\n",
    "\n",
    "    return df_out\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def norm_apply(quantiles, test, num_col, cols):\n",
    "    def normalize_log(column, q):\n",
    "            column_safe = column.clip(lower=0)  # Replace negative values with 0\n",
    "            q_safe = max(q, 1e-6)  # Ensure quantile is not too small\n",
    "            return np.log1p(column_safe + 1) / np.log1p(q_safe + 1)\n",
    "    norm_test_df = test[num_col].apply(lambda col: normalize_log(col, quantiles[col.name]), axis = 0)\n",
    "    norm_test_df = pd.concat([norm_test_df, test[cols]],axis = 1)\n",
    "    return norm_test_df\n",
    "\n",
    "def _inverse_norm(norm_df, pred, quantiles, num_col, predicted_cols):\n",
    "    norm_df_denorm = pd.concat([norm_df, pred], axis = 1)\n",
    "    def inverse_normalize_log(normalized_column, q):\n",
    "        q_safe = max(q, 1e-6)  # Ensure q is not too small\n",
    "        out = np.expm1(normalized_column * np.log1p(q_safe+1))-1 \n",
    "        out[out<=0] = 0\n",
    "        return out  # Apply the inverse transformation\n",
    "    norm_df_denorm[num_col] = norm_df_denorm[num_col].apply(lambda col:\n",
    "                                                            inverse_normalize_log(col, quantiles[col.name]), \n",
    "                                                            axis = 0)\n",
    "    for i in range(1,len(predicted_cols)+1):\n",
    "        norm_df_denorm[predicted_cols[i-1]] = inverse_normalize_log(pred[f'calc_inbound_lead_{i}_predicted'],\n",
    "                                                                  quantiles[f'calc_inbound_lead_{i}'])\n",
    "    return norm_df_denorm\n",
    "\n",
    "def smape(A,F):\n",
    "    val = 2*np.abs(F-A)/(np.abs(A) + np.abs(F))\n",
    "    len_ = np.count_nonzero(~np.isnan(val))\n",
    "    if len_== 0 and np.nansum(val) == 0:\n",
    "        return 100\n",
    "    return 100/len_*np.nansum(val)\n",
    "\n",
    "\n",
    "sales_df,inbound, cal_df, event_df, inv_df, item_univ = _data_input()\n",
    "predict_week = sales_df.wm_yr_wk.max()\n",
    "cal_df = cal_df[cal_df.wm_yr_wk <= predict_week]\n",
    "df_prep = _data_prep(sales_df,inbound, cal_df, inv_df, event_df, group_cols)\n",
    "df_missing = df_prep.copy()\n",
    "df_missing = _data_cleaning(df_missing)\n",
    "\n",
    "# atleast one year of data\n",
    "valid_grp = df_missing.groupby(cat_group)['calc_inbound'].count()\n",
    "print(np.median(valid_grp), np.mean(valid_grp))\n",
    "valid_grp = valid_grp[valid_grp>10].index\n",
    "testing = df_missing[df_missing[cat_group].apply(tuple, axis=1).isin(valid_grp)]\n",
    "\n",
    "univ = item_univ[cat_group].drop_duplicates()\n",
    "univ['wm_yr_wk'] = predict_week\n",
    "\n",
    "temp = df_missing.copy()\n",
    "temp['min_week'] = temp.groupby(cat_group)['wm_yr_wk'].transform(lambda x:x.min())\n",
    "temp['max_week'] = predict_week\n",
    "subcats = temp[cat_group].drop_duplicates()\n",
    "subcats = pd.merge(subcats, cal_df['wm_yr_wk'], how = 'cross')\n",
    "\n",
    "temp = subcats.merge(temp, on = group_cols, how = 'left')\n",
    "temp = temp[(temp['wm_yr_wk'] >= temp['min_week']) & \n",
    "            (temp['wm_yr_wk'] <= temp['max_week'])]\n",
    "\n",
    "univ_val = univ.merge(temp, right_on = group_cols, left_on = group_cols, how = 'outer')\n",
    "univ_val = univ_val[~univ_val.wm_yr_wk.isna()]\n",
    "univ_val = univ_val[~univ_val.r2d2_sub_category_id.isna()]\n",
    "\n",
    "params_feature = {\n",
    "    'lag': np.arange(1, 13, 1),  # Lags from 1 to 3\n",
    "    'lead': [],\n",
    "    'forecast_horizon': np.arange(1, 27, 1),  # Forecasting up to 13 weeks ahead\n",
    "    'rolling_window': [4, 12],  # Rolling window size\n",
    "\n",
    "    # Feature sets\n",
    "    'lag_features': [\n",
    "        'sale_qty_new', 'calc_inbound',\n",
    "        'unit_price_amt_w',\n",
    "        'tot_capacity', 'order_qty'\n",
    "    ],\n",
    "    'lead_features': [\n",
    "        'sale_qty_new'], # sale has to be replaced demand\n",
    "    'target_features': ['calc_inbound'],\n",
    "    'ly_features': [\n",
    "        'sale_qty_new', 'calc_inbound'],\n",
    "\n",
    "    # Categorical grouping\n",
    "    'cat_group': cat_group\n",
    "}\n",
    "df_feats = _build_feature(univ_val, params_feature['cat_group'], params_feature['lag_features'], params_feature['lead_features'], params_feature['lag'],\n",
    "                          params_feature['lead'], params_feature['ly_features'], params_feature['target_features'], \n",
    "                          params_feature['forecast_horizon'], params_feature['rolling_window'], time_col = 'wm_yr_wk')\n",
    "\n",
    "df_feats = generate_hierarchical_features(\n",
    "    df=df_feats,  # stacked train + test\n",
    "    hierarchy_cols=[\n",
    "        'r2d2_sub_category_id',\n",
    "        'r2d2_category_id',\n",
    "        'r2d2_department_id',\n",
    "        'r2d2_super_department_id',\n",
    "        'r2d2_division_id'\n",
    "    ],\n",
    "    time_col='wm_yr_wk',\n",
    "    features_to_agg=['calc_inbound', 'sale_qty_new', 'unit_price_amt_w'],\n",
    "    aggs=['mean', 'sum'],\n",
    "    rolling_windows=[4],\n",
    "    lags=[1, 2, 4],\n",
    "    prevent_leakage=True \n",
    ")\n",
    "\n",
    "\n",
    "def _one_lagged_featues(df, group_cols, cols):\n",
    "    df = df.sort_values(by = group_cols)\n",
    "    for col in cols:\n",
    "        df[f\"{col}_lag1\"] = df.groupby(group_cols)[col].shift(1)\n",
    "    return df\n",
    "cols = ['OH_Inv','DaysInv', 'DaysOOS']\n",
    "df_feats = _one_lagged_featues(df_feats, params_feature['cat_group'], cols)\n",
    "\n",
    "\n",
    "seasonality_year = (\n",
    "    df_feats.groupby(cat_group + ['year'])[['sale_qty_new', 'calc_inbound']]\n",
    "    .sum()\n",
    "    .rename(columns={'sale_qty_new': 'net_yr_sales', 'calc_inbound': 'net_yr_ib'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Overall aggregates (across all time)\n",
    "seasonality_total = (\n",
    "    df_feats.groupby(cat_group)[['sale_qty_new', 'calc_inbound']]\n",
    "    .sum()\n",
    "    .rename(columns={'sale_qty_new': 'net_sales', 'calc_inbound': 'net_ib'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_feats = df_feats.merge(seasonality_year, on=cat_group + ['year'], how='left')\n",
    "df_feats = df_feats.merge(seasonality_total, on=cat_group, how='left')\n",
    "\n",
    "for d in [df_feats]:\n",
    "    d['seasonality_yr_sales'] = np.round(d['sale_qty_new'] / d['net_yr_sales'], 3)\n",
    "    d['seasonality_yr_ib'] = np.round(d['calc_inbound'] / d['net_yr_ib'], 3)\n",
    "    d['seasonality_sales'] = np.round(d['sale_qty_new'] / d['net_sales'], 3)\n",
    "    d['seasonality_ib'] = np.round(d['calc_inbound'] / d['net_ib'], 3)\n",
    "\n",
    "cols_to_drop = [\n",
    "    'InvQty_Sun', 'InvQty_Mon', 'InvQty_Tue', 'InvQty_Wed', 'InvQty_Thu', 'InvQty_Fri', 'InvQty_Sat','last_year_wk',\n",
    "    'last_2year_wk',\n",
    "    'unit_price_amt', \n",
    "    'sale_qty',\n",
    "    'min_week','max_week',\n",
    "    'OH_Inv','DaysInv', 'DaysOOS',\n",
    "    'tot_capacity', 'calc_inbound', 'order_qty',\n",
    "    'net_yr_sales',  'net_yr_ib', 'net_sales', 'net_ib', \n",
    "]\n",
    "df_feats = df_feats.drop(columns = cols_to_drop)\n",
    "\n",
    "cols = [f'{cat}_encoded' for cat in cat_group] + ['year'] + cat_group + time_col + events_col\n",
    "cols_to_exclude = [] # if we want to exclude some columns\n",
    "num_col = df_feats.columns[~df_feats.columns.isin(cols +cols_to_exclude)]\n",
    "\n",
    "norm_forecast_df = norm_apply(quantiles, df_feats, num_col, cat_group + events_col + time_col)\n",
    "\n",
    "original_cols = [\n",
    "    'r2d2_sub_category_id', 'r2d2_division_id', 'r2d2_super_department_id',\n",
    "    'r2d2_department_id', 'r2d2_category_id', 'year'\n",
    "]\n",
    "\n",
    "from joblib import dump, load\n",
    "label_enc = joblib.load('encoders.pkl')\n",
    "\n",
    "for col in original_cols:\n",
    "    encoder = label_enc[col]\n",
    "    mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "    norm_forecast_df[f\"{col}_encoded\"] = norm_forecast_df[col].map(mapping).fillna(-1).astype(int)\n",
    "\n",
    "def _cyclic_features(df1):\n",
    "    df1['month_sin'] = np.sin((df1.month-1)*(2.*np.pi/12))\n",
    "    df1['month_cos'] = np.cos((df1.month-1)*(2.*np.pi/12))\n",
    "    df1['wk_sin'] = np.sin((df1['wm_yr_wk']%100 -1)*(2.*np.pi/52))\n",
    "    df1['wk_cos'] = np.cos((df1['wm_yr_wk']%100 -1)*(2.*np.pi/52))\n",
    "    return df1    \n",
    "\n",
    "forecast_encoded_df = _cyclic_features(norm_forecast_df)\n",
    "\n",
    "forecast_norm_df = forecast_encoded_df[forecast_encoded_df.wm_yr_wk == predict_week]\n",
    "forecast_norm_df1 = forecast_norm_df.drop(columns = target_cols + cat_group + time_col)\n",
    "forecast_norm_df1.fillna(-999, inplace=True)\n",
    "\n",
    "# 26_week_forecast\n",
    "model26 = load('26_week_forecast_0619.joblib')\n",
    "\n",
    "\n",
    "y_forecast = pd.DataFrame(model26.predict(forecast_norm_df1), columns = predicted_cols, index = forecast_norm_df1.index)\n",
    "\n",
    "y_forecast_denorm1 = _inverse_norm(forecast_norm_df, y_forecast, quantiles, num_col, predicted_cols)\n",
    "\n",
    "full_set = univ.merge(y_forecast_denorm1, on = cat_group, \n",
    "                           how = 'left').rename(columns = {'wm_yr_wk_x':'wm_yr_wk'}).drop(columns = ['wm_yr_wk_y'])\n",
    "\n",
    "full_set1 = full_set[group_cols + predicted_cols]\n",
    "\n",
    "full_set1[predicted_cols] = full_set1.groupby(['r2d2_division_id',\n",
    "                               'r2d2_super_department_id',\n",
    "                               'r2d2_department_id',\n",
    "                               'r2d2_category_id',\n",
    "                               'wm_yr_wk'], as_index = False)[predicted_cols].transform(lambda x:x.fillna(x.mean()))\n",
    "\n",
    "full_set1[predicted_cols] = full_set1.groupby(['r2d2_division_id',\n",
    "                               'r2d2_super_department_id',\n",
    "                               'r2d2_department_id',\n",
    "                               'wm_yr_wk'], as_index = False)[predicted_cols].transform(lambda x:x.fillna(x.mean()))\n",
    "\n",
    "full_set1[predicted_cols] = full_set1.groupby(['r2d2_division_id',\n",
    "                               'r2d2_super_department_id',\n",
    "                               'wm_yr_wk'], as_index = False)[predicted_cols].transform(lambda x:x.fillna(x.mean()))\n",
    "\n",
    "full_set1[predicted_cols] = full_set1.groupby(['r2d2_division_id',\n",
    "                               'wm_yr_wk'], as_index = False)[predicted_cols].transform(lambda x:x.fillna(x.mean()))\n",
    "\n",
    "full_set1[predicted_cols] = full_set1.groupby(['wm_yr_wk'], as_index = False)[predicted_cols].transform(lambda x:x.fillna(x.mean()))\n",
    "\n",
    "\n",
    "# Filter historical data to the appropriate window\n",
    "window_size = 200\n",
    "history_df = inbound[\n",
    "    (inbound.wm_yr_wk < predict_week) & \n",
    "    (inbound.wm_yr_wk >= predict_week - window_size)\n",
    "]\n",
    "\n",
    "disagg_level = history_df.groupby(['r2d2_sub_category_id','network_type'\n",
    "                                  ])['calc_inbound'].sum().reset_index(name = 'network_type_total')\n",
    "disagg_level['sub_cat_total'] = disagg_level.groupby(['r2d2_sub_category_id'\n",
    "                                                     ])['network_type_total'].transform(lambda x:x.sum())\n",
    "disagg_level['proportions'] = disagg_level['network_type_total']/disagg_level['sub_cat_total']\n",
    "\n",
    "all_combinations = item_univ[['r2d2_sub_category_id','network_type']].drop_duplicates()\n",
    "all_combinations['network_type_count'] = all_combinations.groupby(['r2d2_sub_category_id'\n",
    "                                                                  ])['network_type'\n",
    "                                                                    ].transform(lambda x:x.nunique())\n",
    "print(all_combinations)\n",
    "full_proportions = all_combinations.merge(\n",
    "        disagg_level[['r2d2_sub_category_id','network_type', 'proportions']], \n",
    "        on=['r2d2_sub_category_id','network_type'], \n",
    "        how='left'\n",
    "    )\n",
    "full_proportions = full_proportions[~full_proportions.network_type.isna()]\n",
    "\n",
    "full_proportions['proportions'].fillna(0, inplace=True)\n",
    "full_proportions['promotion_sum'] = full_proportions.groupby(['r2d2_sub_category_id'\n",
    "                                                             ], as_index = False)['proportions'\n",
    "                                                                                 ].transform(lambda x:x.sum())\n",
    "\n",
    "def calculate_final_proportion(row):\n",
    "    if row['promotion_sum'] > 0:\n",
    "        # Normalize proportions to ensure they sum to 1\n",
    "        return row['proportions'] / row['promotion_sum']\n",
    "    elif row['network_type_count'] > 0:\n",
    "        # Use equal distribution if no historical data\n",
    "        return 1.0 / row['network_type_count']\n",
    "    else:\n",
    "        return 1.0\n",
    "full_proportions['promotion_sum'] = pd.to_numeric(full_proportions['promotion_sum'], errors='coerce').fillna(0)\n",
    "full_proportions['network_type_count'] = pd.to_numeric(full_proportions['network_type_count'], errors='coerce').fillna(0)\n",
    "\n",
    "full_proportions['final_proportion'] = full_proportions.apply(calculate_final_proportion, axis=1)\n",
    "\n",
    "result = full_set1[['r2d2_sub_category_id','wm_yr_wk']+\n",
    "                   predicted_cols].merge(full_proportions, on = ['r2d2_sub_category_id'], how = 'left')\n",
    "\n",
    "for val in predicted_cols:\n",
    "    result[f'{val}'] = result[val]*result['final_proportion']\n",
    "\n",
    "result = result.drop_duplicates()\n",
    "\n",
    "output_col = [predict_week+i for i in range(1,27)]\n",
    "\n",
    "y_forecast_denorm = result.rename(columns=dict(zip(predicted_cols, output_col)))\n",
    "\n",
    "forecasted_output = y_forecast_denorm.melt(id_vars=[col for col in y_forecast_denorm.columns if col not in output_col],\n",
    "                                           value_vars = output_col,\n",
    "                                           var_name = 'wm_yr_wk_1',\n",
    "                                           value_name = 'demand')\n",
    "\n",
    "forecasted_output.rename(columns = {'wm_yr_wk':'start_date',\n",
    "                                    'wm_yr_wk_1':'wm_yr_wk',\n",
    "                                    'r2d2_sub_category_id': 'item_hierarchy_id'}, inplace = True)\n",
    "\n",
    "today = pd.Timestamp.today()\n",
    "friday = today + pd.offsets.Week(weekday=4)\n",
    "\n",
    "# If today is after Friday, offset goes to next week's Friday, so adjust:\n",
    "if today.weekday() > 4:\n",
    "    friday = today - pd.Timedelta(days=today.weekday() - 4)\n",
    "else:\n",
    "    friday = today + pd.Timedelta(days=4 - today.weekday())\n",
    "\n",
    "# Set the column\n",
    "friday = friday.strftime('%Y-%m-%d')\n",
    "\n",
    "forecasted_output['item_hierarchy_type']  = 'sub_cat_id'\n",
    "forecasted_output['start_date'] = friday\n",
    "\n",
    "dummy = forecasted_output[['item_hierarchy_id','item_hierarchy_type','network_type','wm_yr_wk','demand','start_date']]\n",
    "\n",
    "dummy = dummy.groupby(['item_hierarchy_id', 'item_hierarchy_type', 'network_type', 'wm_yr_wk','start_date'], as_index = False)['demand'].mean()\n",
    "\n",
    "dummy['start_date'] = dummy['start_date'].astype(str)\n",
    "dummy.to_parquet('gs://inbound-forecast-sn/inbound_national_forecast_PANDAS/start_date='friday'/inbound_forecast.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
